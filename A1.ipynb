{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fzMyE386oaaV"
   },
   "source": [
    "**Fall 2019**\n",
    "\n",
    "**P556: Applied Machine Learning**\n",
    "\n",
    "**Assignment #1**\n",
    "\n",
    "**Due date: September 18, 2019. 11:59 PM**\n",
    "\n",
    "DO NOT CHANGE THE FUNCTION DEFINITIONS UNLESS APPROVED BY AN AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FJz8ZxU7opMy"
   },
   "source": [
    "# Problem #1: Linear Regression\n",
    "\n",
    "##  Problem 1.1 (25 points)\n",
    "\n",
    "Implement linear regression using gradient descent. Your implementation should be able to handle simple and multiple linear regression.\n",
    "\n",
    "Note 1: by implementation we mean that everything has to be written from scratch and that you cannot call a linear regression function from a library, such as sklearn. Usage of standard libraries, such as numpy, pandas, etc., is fine. If you are unsure about whether a library can be used, please contact the AIs well in advance of the submission date.\n",
    "\n",
    "Note 2: You are free to use sklearn to test whether your results match that from a battle-tested library. This is a great way to know before hand whether your submission is correct. Make sure to use the same parameters on both models before you spend an eternity debugging code that is correct but not returning the same values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dW1xPyXPoonO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "class linear_regression:\n",
    "    def __init__(self, learning_rate, iterations, \n",
    "               fit_intercept=True, normalize=False, coef=None):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.normalize = normalize\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.coef = coef\n",
    "  \n",
    "    def fit(self,X, y):\n",
    "        \"\"\"\n",
    "        Fit linear model.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Training data\n",
    "        y : array_like, shape (n_samples, n_targets)\n",
    "            Target values.\n",
    "        \"\"\"\n",
    "\n",
    "        learning_rate = 0.0001      #tried values betwn 0.1 to 0.0001 and got best results for 0.0001   \n",
    "        iterations = 1000           #tried different values between from 200 to 2000 and it got insignificant after 1000\n",
    "\n",
    "        n = len(y)                  #no of tuples in training data\n",
    "        num_col = X.shape[1]        #no of features in the dataset\n",
    "\n",
    "        coef = []                   #declare theta array at the beginning\n",
    "        for i in range(num_col+1):\n",
    "            coef.append(0)          #initialize theta to 0 initially\n",
    "\n",
    "        \"\"\"The functions used in following line have been referenced from numpy manual\"\"\"\n",
    "        X = (X-np.mean(X))/(np.amax(X)-np.amin(X))\n",
    "        \n",
    "        \"\"\"To add 1s as first column in X matrix, the function hstack is used from numpy manual\"\"\"\n",
    "        X0 = np.ones((n,1))\n",
    "        X = np.hstack((X0,X))\n",
    "\n",
    "        for i in range(iterations):\n",
    "            Yp = []\n",
    "            Yp = np.dot(X,coef)           #multiply the X with theta\n",
    "            for i in range(num_col+1):\n",
    "                dummy = 0                 #initialize a dummy variable to use in furthur computations\n",
    "                for j in range(n):\n",
    "                    #dummy variable updated each time with product of X & diff between actual and predicted y value\n",
    "                    dummy = dummy + ((Yp[j]-y[j])*X[j][i])\n",
    "                    #theta updated each time w.r.t dummy and learning rate\n",
    "                    coef[i] = coef[i]-(2*learning_rate*dummy)/n\n",
    "        return (coef)\n",
    "    pass\n",
    "  \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict using the linear model\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array_like, shape (n_samples, n_features)\n",
    "        Samples.\n",
    "        Returns\n",
    "        -------\n",
    "        C : array, shape (n_samples,)\n",
    "            Returns predicted values.\n",
    "        \"\"\"\n",
    "        y = []\n",
    "        \n",
    "        \"\"\"The functions used in following line have been referenced from numpy manual\"\"\"\n",
    "        X = (X-np.mean(X))/(np.amax(X)-np.amin(X))\n",
    "        \n",
    "        \"\"\"To add 1s as first column in X matrix, the function hstack is used from numpy manual\"\"\"\n",
    "        X0 = np.ones((X.shape[0],1))\n",
    "        X = np.hstack((X0,X))\n",
    "        \n",
    "        num_col = X.shape[1]                \n",
    "        for i in range(len(X)):             #consider all the tuples in the X\n",
    "            k = 0\n",
    "            for j in range(num_col):        #consider all the columns in the X\n",
    "                k = k + X[i][j]*coef[j]     #multiply the X_test data with the coef we calculated in fit function\n",
    "            y.append(k)                 \n",
    "        return y\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Man3c1JrhVbr"
   },
   "source": [
    "## Problem 1.2 (10 points)\n",
    "\n",
    "- Split the Boston Housing dataset into train and test sets (70% and 30%, respectively) (5 points). \n",
    "- Fit your linear regression implementation using the training set and print your model's coefficients. Make predictions for the test set using your fitted model (5 points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VEFBL6WwhXUz",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.026926006105825, 22.92228136743191, 11.507472081989668, 24.31622118917274, 16.130565622696423, 25.07910247154768, 26.09573132533813, 25.01820844779173, 17.940091264795004, 24.723974462289178, 24.50701860296393, 24.839479009308302, 26.209658850615078, 22.20554710752627, 21.623759569152774, 21.31760109364329, 24.975266292862447, 25.679838311098784, 23.87707058521482, 24.90327863184643, 17.24195180588063, 25.864482339818046, 24.718032619649982, 24.992084089632453, 24.289199788516502, 23.076022578692086, 24.91186679056463, 24.737628831298494, 26.452394454037485, 15.454578965169302, 24.99216812892567, 24.241613319351803, 22.42880645073489, 25.88416925961673, 18.651529945704993, 18.201148861048114, 12.577367181665524, 22.06688810357779, 23.547001306534504, 25.527672980651346, 24.688748679611034, 17.321074610298158, 25.865168025568686, 24.187141066550158, 24.360529502097048, 24.36280065508872, 23.173568277138198, 23.079609490683975, 25.480140383940054, 21.810688847342327, 17.66402265082654, 24.268229195493568, 25.18826670381552, 23.291984382923474, 25.84099369687109, 24.59579228568305, 22.973349376144622, 25.81560068250955, 25.657097618386185, 24.302236446963526, 24.433864210537468, 18.181330715985023, 25.988753300686806, 25.006419715572843, 24.381168885354487, 24.02485309765283, 18.150608473317725, 21.943378166942175, 23.946273003806045, 23.22173426170996, 23.930162856398976, 25.10320134072963, 24.562417462545984, 18.32559933533868, 24.157112098610938, 18.27482952414383, 23.12434533635734, 25.825004823411064, 18.104474872718274, 25.145439147729526, 23.46122733876047, 23.811060603574262, 24.026816885352567, 24.964226459244543, 21.328608585791283, 23.248972222557327, 17.849540930641343, 23.11936006688507, 23.188029531097403, 17.646825733524192, 24.371353575374723, 22.36059262485162, 24.222600004254133, 24.204205834565872, 23.702493801287364, 24.428595681328847, 24.272834090778304, 24.93922453013668, 20.841101348698764, 24.86192338250085, 18.0482634944096, 24.41535110402455, 24.878039189156326, 22.159179493525595, 18.217976174113574, 25.85417297571306, 18.241111518637116, 11.29377376082601, 17.43858869016181, 15.814648739930902, 24.8730963317002, 25.0429783898233, 25.6276781160865, 18.213466335260375, 19.80355997288556, 18.089596528565206, 18.375721594292095, 18.383960060765304, 24.626289313372485, 17.836433724644607, 25.864877435862702, 22.66988342399831, 18.146549689754224, 18.322713480186238, 22.1064103376973, 22.98346537846259, 24.221708037838887, 25.969772414057612, 11.85393599389785, 22.023751009683842, 22.154387679166927, 24.530972627770204, 25.12190071329187, 24.707779967761347, 25.76938322223029, 21.955763216304906, 24.074677402984467, 25.165300621201197, 17.99530366357298, 22.234071517199837, 25.54865029808198, 11.62566502631588, 24.1170135057633, 24.482367094889106, 24.93699606282177, 23.937372584224477, 17.722870724586166, 25.588096865129994, 22.972389933231977, 25.833377172704896, 19.956233273669028, 24.104724335108852]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Your code goes here\n",
    "data = load_boston()\n",
    "X, y = data['data'], data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3)\n",
    "coef = linear_regression.fit(1,X_train,y_train)\n",
    "y_pred = linear_regression.predict(1,X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9FXqtD_KhZwV"
   },
   "source": [
    "## Problem 1.3 (10 points)\n",
    "\n",
    "Identify the variable or set of variables that will minimize the mean square error (MSE). Hint: this is where your function being able to handle simple and multiple regression becomes useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sFlcvY_piKus"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.59410629564235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\"var=[]\\nfor i in range(X.shape[1]):\\n    Xi = []\\n    for j in range(X.shape[0]):\\n        Xi.append([j][i])\\n    var.append(np.var(X[j][i]))        \\nprint (var)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "mse = (mean_squared_error(y_test, y_pred))\n",
    "print(mse)\n",
    "\n",
    "\"\"\"\"var=[]\n",
    "for i in range(X.shape[1]):\n",
    "    Xi = []\n",
    "    for j in range(X.shape[0]):\n",
    "        Xi.append([j][i])\n",
    "    var.append(np.var(X[j][i]))        \n",
    "print (var)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KcqYa1U3iRQ1"
   },
   "source": [
    "## Problem 1.4 (5 points)\n",
    "\n",
    "1. How do you interpret that a variable causes a model's mean square error to increase? (2 points)\n",
    "  \n",
    "     - Answer: Mean Square error is basically how far the predicted values are present from the actual values. There are two contributing factors associated to MSE, bias and variance! The bias is how far all the predicted values are from the actual values. It is the assumptions made to make easier for model to fit to the target values! The variance is how much target values change with the change in the training data. It tells us how the model can perform well for given training data and might not be that accurate for different training set. The mean square error increases with the increase in either bias or variance. Mathematically it can be simply represented as MSE = (Bias)^2 + Variance. In such ways, the variable can cause the mean square error to increase. So to reduce the mean square error, you can't reduce bias and variance at the same time. Hence there is a certain tradeoff between bias and variance to achieve a better model or a least possible mean squared error.\n",
    "      \n",
    "      \n",
    "2. Why we would want to normalize our variables? (1 point)\n",
    "      \n",
    "    - Answer: Any given dataset has different features and these features have different values in different ranges depending on what the feature is. It is very complicated to work on such data and make predictions as one feature might have all values in single digit and the other feature may have all values as three digit numbers.For e.g, in any housing dataset the feature #bedrooms will always have a value at the max 10, whereas the feature area of house may have values ranging 1000-4000. So its necessary to normalize the variables and bring them to common scale in order to perform computations on the data!\n",
    "\n",
    "\n",
    "3. A model fitted using the exact same split dataset with normalized values will generate the same coefficients as a model that was fitted using values that haven't been normalized. Clearly state whether that statement is true or false and explain your reasoning. (2 points)\n",
    "  \n",
    "   - Answer: The given statement is false. When the coeffecient is calculated for the split dataset with values which are not normalized, it will do all the computations on the actual values of the data. The coefficient is updated for each iteration depending upon the values of the given dataset. However when we normalize the same set of split dataset, those values are scaled to a particular range. Now the coefficient is calculated with respect to these values and in accordance with this new scale. Also, I checked this by experimenting it in my code once and thus cross verified it on actual dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RJSot41BkMrB"
   },
   "source": [
    "# Problem 2: Binary Classification\n",
    "\n",
    "## Problem 2.1 (5 points)\n",
    "\n",
    "Consider the binary classification problem of mapping a given input to two classes. Let $\\mathcal{X}=\\mathbb{R}^d$ and $\\mathcal{Y}=\\{+1, -1\\}$ be the input space and output space, respectively. In simple words, it means that the input has $d$ features and all of them are real valued, whereas the output can only take values $-1$ or $+1$. This is one of the most common problems in machine learning and many sophisticated methods exist to solve it. In the question, we will solve it using the concepts we have already learned in class. Let us assume the two sets of points can be separated using a straight line i.e. the samples are linearly separable. So if $d=2$, one should be able to draw a line to distinguish between the two classes. All points lying on side of the line should belong to a particular class (say $1$) and the points lying on the other side should belong to another class (say $2$). To see what this would look like,  your first task is as follows:\n",
    "\n",
    "Write a function that will randomly generate a dataset for this problem. Your function should randomly choose a line $l$, which can be denoted as $ax + by + c = 0$. According to basic high school geometry, the line divides the plane into two sides. On one side, $ax+by+c>0$ while on the other $ax+by+c<0$. Use this fact to randomly generate $k_0$ points on the side of class 0 (i.e. $y=-1$) and $k_1$ points on the side of class 1 (i.e. $y=1$). Create a plot of this dataset where all the points corresponding to one class are blue and those of the other class are green, the line dividing both classes should be red. Axes should be labeled.\n",
    "\n",
    "**Note**: Do not confuse the $x$ and $y$ in the equation of line $ax + by + c = 0$ with $\\mathcal{X} $ and $\\mathcal{Y}$. Instead imagine these $x$ and $y$ as the 2-D coordinate system on which you have different points which should lie on 2 sides of the line $ax + by + c = 0$. For example, there is a point (2,3) in the 2-D system where $x = 2$ and $y = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g96jFpGyMIFu"
   },
   "outputs": [],
   "source": [
    "def generate_dataset(k0, k1):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    k0 : integer, number of samples for class 0\n",
    "    k1 : integer, number of samples for class 1\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X : array, shape (m, d), dimension numpy array where m is the number of \n",
    "    samples and d is the number of features \n",
    "\n",
    "    Y : array, (m, 1), dimension vector where m is the number of samples\n",
    "    \"\"\"\n",
    "\n",
    "    from matplotlib import pyplot as plt\n",
    "    import random\n",
    "    import numpy as np\n",
    "\n",
    "    x = np.linspace(-20,20,10)\n",
    "    a = random.randint(-20,20)\n",
    "    b = random.randint(-20,20)\n",
    "\n",
    "    #Below condition is inserted when b is selected as 0, it gave me error as b is a denominator in eqn of line\n",
    "    while b==0:                      \n",
    "        b=random.randint(-20,20)\n",
    "\n",
    "    c = random.randint(-20,20)\n",
    "    z = []\n",
    "    while (len(z)<k0):                  #generate k0 points\n",
    "        p = random.randint(-20,20)\n",
    "        q = random.randint(-20,20)\n",
    "        if(((a*p)+(b*q)+c)<0):          #check the condition ax+by+c<0 so that k0 points belong to class 0\n",
    "            z.append((p,q))\n",
    "    for i in range(k0):\n",
    "        plt.scatter(z[i][0],z[i][1],color='blue')   #plot all those k0 points belonging to class 0 as blue color\n",
    "\n",
    "    r = []\n",
    "    while (len(r)<k1):                 #generate k1 points\n",
    "        p = random.randint(-20,20)\n",
    "        q = random.randint(-20,20)\n",
    "        if(((a*p)+(b*q)+c)>0):         #check the condition ax+by+c>0 so that k1 points belong to class 1\n",
    "            r.append((p,q))      \n",
    "    for i in range(k1):\n",
    "        plt.scatter(r[i][0],r[i][1],color='green')  #plot all those k1 points belonging to class 1 as green color\n",
    "\n",
    "    y = (a*x+c)/(-1*b)\n",
    "    plt.plot(x, y, '-r')         #plot the random line ax+by+c=0 where a,b,c are random\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    X=z+r                   #X is input space with all k0 and k1 points\n",
    "    for i in range(k0):\n",
    "        y.append(-1)        #y will be -1 if it belongs to class 0\n",
    "    for i in range(k1):\n",
    "        y.append(1)         #y will be 1 if it belongs to class 1\n",
    "    X=z+r\n",
    "    return(X,y)\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9mw15CFikXI1"
   },
   "source": [
    "## Problem 2.2 (35 points)\n",
    "\n",
    "If $\\mathcal{Y}$ is the variable you are trying to predict using a feature $\\mathcal{X}$ then in a typical Machine Learning problem, you are tasked with a target function $f$ which maps $\\mathcal{X}$ to $\\mathcal{Y}$ i.e. Find $f$ such that  $\\mathcal{Y}$  = $f(\\mathcal{X})$\n",
    "\n",
    "\n",
    "When you are given a dataset for which you do not have access the target function $f$, you have to learn it from the data. In this problem, we are going to learn the parameters of the line that separates the two classes for the dataset that we constructed in Problem 2.1. As we previously mentioned, that line can be represented as $ax + by + c = 0$.\n",
    "\n",
    "The goal here is to correctly find out the coefficients $a$, $b$, and $c$, represented below as $\\bf{w}$ which is a vector. The algorithm to find it is a simple iterative process: \n",
    "\n",
    "1. Randomly choose a $\\mathbf{w}$ to begin with.\n",
    "2. Keep on adjusting the value of $\\bf{w}$ as follows until all data samples are correctly classified:\n",
    "    1. Randomly choose a sample from the dataset without replacement and see if it is correctly classified. If yes,  move on to another sample.\n",
    "    2. If not,  then  update the weights as $\\mathbf{w}^{t+1} = \\mathbf{w}^t + y \\cdot \\mathbf{x}$\n",
    "    and go back to the previous step (of randomly chosing a sample)\n",
    "    \n",
    "        - $\\mathbf{w}^{t+1}$ is value of $\\mathbf{w}$ at iteration $t+1$\n",
    "        - $\\mathbf{w}^{t}$ is value of $\\mathbf{w}$ at iteration $t$\n",
    "        - $y$ is the class label for the sample under consideration\n",
    "        - $\\mathbf{x}$ is the data-point under consideration\n",
    "    \n",
    "    \n",
    "Write a function that implements this learning algorithm. The input to the function is going to be a dataset represented by the input variable $X$ and the target variable $y$. The output of the function should be the chosen $\\mathbf{w}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPk7AZaLkXSh",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The weight vector is :\n",
      "[-75, -40]\n"
     ]
    }
   ],
   "source": [
    "def fit_line(X, y):\n",
    "    \"\"\"Predict using the binary classification model. Use the dataset generated \n",
    "    using generate_data() as input for this function.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array_like, shape (n_samples, n_features)\n",
    "        Samples.\n",
    "    y : array_like, shape (n_labels, 1)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    w : array, shape (1,n_features)\n",
    "        Returns the final weight vector w.  \n",
    "    \"\"\"\n",
    "    import random\n",
    "    \n",
    "    num_col = (len(X[0]))                #number of features i.e. features in X\n",
    "    w = []\n",
    "    for i in range(num_col):\n",
    "        w.append(random.randint(0,10))   #initialize weight vector with random values\n",
    "    \n",
    "    yp = []\n",
    "    for i in range(len(y)):\n",
    "        yp.append(0)                     #initialize the y-predicted list to zero              \n",
    "    \n",
    "    list = []\n",
    "    #below while condition ensures that all the samples are picked up\n",
    "    while(len(list)<len(y)):\n",
    "        i = random.randint(0,len(y)-1)   #choose any random row from X\n",
    "        if i not in list:\n",
    "            #y = w1x1 + w2x2 + w3x3......wnxn where w is weight vector and Xi depends on no. of features\n",
    "           \n",
    "            y_out = 0                         #initialize a dummy variable with 0\n",
    "            for j in range(num_col):      \n",
    "                y_out = y_out + w[j]*X[i][j]  #multiply the weight vector w with the corresponding X values\n",
    "            \n",
    "            #The output space should be [-1,1]\n",
    "            if y_out<0:\n",
    "                yp[i]=-1\n",
    "            else:\n",
    "                yp[i]=1\n",
    "                \n",
    "            #If the predicted value is not equal to actual value, update the weights\n",
    "            if yp[i]!=y[i]:\n",
    "                for j in range(num_col):\n",
    "                    w[j]= w[j]+(y[i]*X[i][j])    #𝐰(𝑡+1) = 𝐰𝑡 + 𝑦⋅𝐱 \n",
    "            else:\n",
    "                list.append(i)  #it will append the sample to list only if it has been correctly classified    \n",
    "    return(w)\n",
    "pass\n",
    "\n",
    "# Main Function\n",
    "if __name__ == \"__main__\":\n",
    "    X,y = generate_dataset(300,300)\n",
    "    W = fit_line(X,y)\n",
    "    print('The weight vector is :')\n",
    "    print(W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHc3t4e9MOxu"
   },
   "source": [
    "### Problem 2.3 (10 points)\n",
    "- Give an intuition of why the above algorithm converges for linearly separable data? We do not expect you to give a mathematic proof, but it would be great if you can provide one. You will get full points even if you just provide an intuition of a few lines. Including figures or mathematical equations is encouraged but not required. (5 points)\n",
    "\n",
    "  - Answer: The data we have is linearly seperable data i.e. it does not have any outliers beyond its class. The execution is carried out till all the misclassified tuples are properly classified. At any point in execution, the difference between    |Wk-Wi| is bounded by a function say Ck where C is constant. Also as the perceptron algorithm proceeds, the number of misclassifications k approaches close to function. The overall result follows from this bound and the fact that, at each iteration until convergence, the perceptron classifies at least one vector each time. Also as there are only two classes possible for the output space, and as we take the dot product of the weight vector and the features the cosine has just the possibilities of either less than or greater than 90. It will go on till all the tuples are correctly classified and this is achieved as the data is linearly seperable. \n",
    "\n",
    "- What happens when the data is not linearly separable? What can be done to salvage the situation?\n",
    "\n",
    "  - Answer: When the data is not linearly seperable, there is no gurantee that the algorithm will convergence nor any gurantee of the accuracy. In above code, we specify to classify each tuple till all are correctly classified. It happens because the data is linearly seperable but it won't work in case of linearly inseperable data. In order to solve this problem, we can provide some condition to the algorithm to help determine when it should stop trying to fit the data. The conditions can be anything which would tell the algorithm when to stop. For example, define the number of iterations beforehand or set some minimum value for the number of tuples which are not properly classified. We can also try to extend it in higher dimensional space so that the data might be linearly seperable in higher dimension than the lower one."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "A1-F19.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
